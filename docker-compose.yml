# SPDX-FileCopyrightText: 2019-2020 Magenta ApS
# SPDX-License-Identifier: MPL-2.0

version: '3.4'

services:
  # We use two images in development. One frontend that runs `vue-cli-service
  # serve`, named `frontend` and one that runs flask named `mo`. The frontend
  # image proxies requests to flask.

  # In production only the flask image is run. It will serve the production
  # ready frontend code, but the files are only updated on docker build.
  frontend:
    build:
      context: .
      dockerfile: docker/Dockerfile
      target: frontend
    environment:
      # `vue-cli-service serve` will proxy request to `/service` and `/saml`to
      # this endpoint. Defined in `frontend/vue.config.js`.
      - BASE_URL=http://mo:80
    # To reload on file changes, we mount the current folder into `/code`.
    # However we do not want node_modules on the host machine, so we create an
    # additional volume for it.
    volumes:
      - .:/app/
      - ./dev-environment/keycloak.json:/app/frontend/public/keycloak.json
      - /app/frontend/node_modules
    depends_on:
      - mo
    ports:
      - "5001:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 40s

  mo:
    build:
      context: .
      dockerfile: docker/Dockerfile
      target: dist
    # The `volumes` and `command` mounts the backend directory in the docker
    # container and overwrites the CMD from Dockerfile. With this gunicorn
    # reloads on file changes. This is very useful when developing the backend.
    # We also change the Flask app to one where a testing API for testcafe
    # e2e-tests is enabled.
    volumes:
      # Uncomment this with the path you would like to use for queries
      # - /tmp:/queries
      - ./backend:/app/backend
      - ./dev-environment/mo-dev-settings.toml:/user-settings.toml
      - ./.pytest_cache:/app/.pytest_cache
    command: ["/start-reload.sh"]
    environment:
      JAEGER_HOSTNAME: jaeger
      JAEGER_SERVICE: OS2mo
    ports:
      - "5000:80"
    depends_on:
      - mox
      - msg_broker
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80"]
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 40s

  mox:
    image: magentaaps/lora:dev
    # build:
    #   context: ../lora
    #   dockerfile: docker/Dockerfile
    volumes:
    #  - ../mox:/app
      - ./dev-environment/mox-dev-settings.toml:/user-settings.toml
    # command: '/start-reload.sh'
    environment:
      MAX_WORKERS: "1"
      JAEGER_HOSTNAME: jaeger
      JAEGER_SERVICE: LoRa

    ports:
      - "8080:80"
    depends_on:
      - mox-db
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/site-map"]
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 40s

  mox-db:
    image: magentaaps/postgres-os2mo:10-11.7-test
    env_file:
      - dev-environment/db.env
      - dev-environment/conf_db.env
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 30s
    # The following will mount the database data directory to RAM. It
    # significantly speeds up the database at the cost of dataloss on shutdown.
    # This is useful for running tests, but should never be used if you want to
    # save the data.
    # tmpfs:
    #   - /var/lib/postgresql/data

  msg_broker:
    # Normally, we expect the `rabbitmq` image. The -management images come
    # with a set of management plugins installed and enabled by default. They
    # can be accessed through the web interface on port 15672. The credentials
    # are guest/guest.
    image: rabbitmq:3-management
    # RabbitMQ stores data based on what it calls the "Node Name", which
    # defaults to the hostname. We set this, so we can keep track of our data.
    hostname: msg_broker
    # We expose the ports here, so decoupled agents can be developed with more
    # ease.
    ports:
      - "5672:5672"    # amqp port
      - "15672:15672"  # management port

  keycloak:
    image: quay.io/keycloak/keycloak:13.0.0
    # depends_on:
    #   - keycloak-db
    ports:
      - "8081:8080"
    environment:
      KEYCLOAK_USER: admin
      KEYCLOAK_PASSWORD: admin
      KEYCLOAK_IMPORT: /tmp/realm.json
      # DB_VENDOR: POSTGRES
      # DB_ADDR: "keycloak-db"
      # DB_USER: keycloak
      # DB_PASSWORD: keycloak
      # DB_SCHEMA: public
      # DB_DATABASE: keycloak
      # PROXY_ADDRESS_FORWARDING: 'true'
      # KEYCLOAK_FRONTEND_URL: https://proxy/auth
      # KEYCLOAK_LOGLEVEL: DEBUG
    volumes:
      - type: bind
        source: ./dev-environment/keycloak-realm.json
        target: /tmp/realm.json

# Not needed in the dev environment, but kept as an inspiration for production
#  keycloak-db:
#    image: postgres:13.3
#    environment:
#      POSTGRES_DB: keycloak
#      POSTGRES_USER: keycloak
#      POSTGRES_PASSWORD: keycloak

  jaeger:
    image: jaegertracing/all-in-one
    ports:
      - "16686:16686"
